# Alternative approach

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import time

BATCH_SIZE = 50
# With a batch size of 25, our total number of batches/iterations per epoch will be 1200.

#Defining the transforms
transform = transforms.Compose([transforms.ToTensor()])

# Download and load training and testing dataset
train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,
                                        download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,
                                          shuffle=True, num_workers=10)

# Download and load testing dataset
test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False,
                                       download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,
                                         shuffle=False, num_workers=10)
                                         
# Creating a class for the hyper parameters will make life easier when tuning the model 
class HypParam:
    inputs = 64*5*5
    hidden_layer1 = 64
    hidden_layer2 = 32
    hidden_layer3 = 16
    outputs = 10
    learning_rate = 0.01
    num_epochs = 10
    
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.fc = nn.Linear(HypParam.inputs, HypParam.hidden_layer1)
        self.fc1 = nn.Linear(HypParam.hidden_layer1, HypParam.hidden_layer2)
        self.fc2 = nn.Linear(HypParam.hidden_layer2, HypParam.outputs)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.layer3(out)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        out = self.fc1(out)
        out = self.fc2(out)
        return out
        
   # If GPU is available, select GPU, if not then select CPU
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Instantiating
Model_cnn = CNN()
Model_cnn.to(device)
# Optimizer and loss function
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adagrad(Model_cnn.parameters(), lr=HypParam.learning_rate)

# Calculating the accuracy
def acc(loader):
  correct = 0
  total = 0
  for data in loader:
    inputs, labels = data[0].to(device), data[1].to(device)
    outputs = Model_cnn(inputs)
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
  return ((100*correct)/total)

def train():
    epoch_loss = []
    train_acc = []
    test_acc = []
    for epoch in range(HypParam.num_epochs):
        run_loss = 0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
 
            # Set parameter gradients to zero
            optimizer.zero_grad()
 
            # Forward + backward pass
            outputs = Model_cnn(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            run_loss += loss.detach().item()
 
        epoch_loss.append(run_loss/300)
        train_acc.append(acc(train_loader))
        test_acc.append(acc(test_loader))
        
        print('Epoch: %d of %d, Train Acc: %0.3f, Test Acc: %0.3f, Loss: %0.3f'
        % (epoch+1, HypParam.num_epochs, train_acc[epoch], test_acc[epoch], run_loss/1200))
        
    return epoch_loss, train_acc, test_acc
    
  
start = time.time()
epoch_loss, train_acc, test_acc = train()
end = time.time()
 
print('%0.2f minutes' %((end - start) / 60))
